{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Install libraries\n",
    "\n",
    "%pip install tensorflow\n",
    "# deep learning library used to creat sequential models\n",
    "\n",
    "!pip install pandas\n",
    "# read data in tabular format since it is in excel or csv format\n",
    "\n",
    "!pip install matplotlib\n",
    "#help with plotting\n",
    "!pip install scikit-learn\n",
    "#used basically for matrix\n",
    "Import requierd libraries\n",
    "import os #help to work with different file paths or navigate through different file paths\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "Bring in our data\n",
    "df = pd.read_csv(  # used to read in our csv\n",
    "    os.path.join(r'C:\\Users\\FALGUNI\\OneDrive\\Documents\\Hate comment detection\\jigsaw-toxic-comment-classification-challenge\\train.csv.zip'))\n",
    "    #pass the path of the training data we are going to use\n",
    "\n",
    "df.head()\n",
    "#to read data from start of our file \n",
    "df.tail()\n",
    "#to read data from the end of our file\n",
    "df.iloc[6]['comment_text']\n",
    "#to read particular comment\n",
    "df[df.columns[2:]].iloc[6]\n",
    "# to look at perticular comment attribute values\n",
    "Preprocessing of the data\n",
    "\n",
    "Tokenizing the data --> translating the sentences into our own secret language that only deep learning network understand\n",
    "i.e. convert each word in the sentence into a unique identifier so that each word maps to a number\n",
    "from tensorflow.keras.layers  import TextVectorization\n",
    "# using tectvectorization to tokenize our text\n",
    "split the data into comments and features\n",
    "x = df['comment_text']\n",
    "y = df[df.columns[2:]].values\n",
    "#  we can customize our output as we need to display only the index values use df.columns this will list all columns of table. If we need all values for particular column then type df['comment_text'] this will give us all the comments\n",
    "\n",
    "#to get all columns from 2nd and use .values to represent in the form of an array. .values will give us the format which we can pass to out tenserflow deep learning model.\n",
    "\n",
    "#so under X we have all our bad comments and under Y we have all our labels\n",
    "\n",
    "df[df.columns[2:]].values\n",
    "#printing all columns that conatain label.\n",
    "MAX_FEATURES = 200000\n",
    "# this will specify the number of words in the vocalobary our dictnary will have i.e. how many words will be stored in dict. in the form of textvectorization.\n",
    "initialize textvectorization layer\n",
    "vectorizer = TextVectorization(max_tokens = MAX_FEATURES, #max words in out vocab should be this \n",
    "                               output_sequence_length = 1800, #maximum length of the sentence in the token\n",
    "                               output_mode = 'int') \n",
    "teach our vectorizer our vocab i.e. train the model with our vocab\n",
    "vectorizer.adapt(x.values)\n",
    "#adapt will learn all our words inside our vocablory\n",
    "#to see the effect of vectorizer we can simply try this out\n",
    "vectorizer('Hello world, life is great')[:5]\n",
    "#this will give us the values for each word of the above sentence in the form of array, so 288 is Hello and so on.\n",
    "do above effect for all comments rather than just above sentence\n",
    "vectorizer.get_vocabulary()\n",
    "vectorized_text = vectorizer(x.values)\n",
    "#hence after above vectorization our data now looks like this\n",
    "#so we have total 159571 samples and 1800 words are tokenized as we mentioned before in limit \n",
    "#also we can observe in the output that if perticular sentence does not meet the limit here 1800 words then it will just fill 0's in place of it.\n",
    "vectorized_text\n",
    "create our dataset \n",
    "#create tensorflow data pipeline--> this will make it easy to train deep learning model and is mainly used for the data which cannot fit into the memory \n",
    "\n",
    "#MCSHBAP - map, chache, shuffel, batch, prefetch\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((vectorized_text, y)) #create a dataset, we pass our data we created above and  also the vectorized text\n",
    "dataset = dataset.cache() #caches our data \n",
    "dataset = dataset.shuffle(160000) #pass the argument as how large should be the buffer size\n",
    "dataset = dataset.batch(16) #each batch is represented as series of 16 samples \n",
    "dataset = dataset.prefetch(8) #help prevents bottlenecks\n",
    "grab data from dataset \n",
    "dataset.as_numpy_iterator().next()\n",
    "#this will represent our comment/text/sentences in vectorized format with all the labels \n",
    "#unpack above data as \n",
    "batch_x, batch_y = dataset.as_numpy_iterator().next()\n",
    "#vectorized text examples -->we can see we have 16 samples and maxed as 1800 words each \n",
    "batch_x.shape\n",
    "#labels --> 16 samples represented by 6 values in the vector\n",
    "batch_y.shape\n",
    "create our training validation and test partetation \n",
    "train = dataset.take(int(len(dataset)*.7)) #take 70% of the length of our dataset i.e. assign 70% of the data to training partetion.\n",
    "# len(dataset) will print the length of data(it will be in batches since we converted before). \n",
    "# len(dataset)*.7 will give 70% of that data. \n",
    "#int(len(dataset)*.7) will convert it into integer\n",
    "#dataset.take will take this pertationed data out and assign it to variable train\n",
    "val = dataset.skip(int(len(dataset)*.7)).take(int(len(dataset)*.2))\n",
    "#.skip specifies that skip 70% of the data i.e. used in train and then take another 20% as our validation partation\n",
    "test = dataset.skip(int(len(dataset)*.9)).take(int(len(dataset)*.1))\n",
    "#here skip 70+20 = 90% of the data and remaining 1% used as testing data \n",
    "train.as_numpy_iterator().next()\n",
    "#take the batch out of data as dataset name (train here).as_numpy_iterator which will create a generator and .next will get us the next batch \n",
    "Embedding the tokens.\n",
    "It's like a personality test for a word. It tells us all about that word which will make it very useful for deep learnning\n",
    "\n",
    "1st step is to create sequential model\n",
    "from tensorflow.keras.models import Sequential\n",
    "#first import the sequential APIs since it is fastest and easiest of all other methods\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Bidirectional, Dense, Embedding\n",
    "#import the layers which we will use to build on our deep neuron network\n",
    "#first layer will be lstm layer and bi-directional will be a modifier on top of it and will allow us to pass the features/values from lstm output across the board as we pass through the sequences \n",
    "#dropout is method of regularization and dense layer is our fully connected layer \n",
    "# Define the input shape\n",
    "INPUT_LENGTH =  None\n",
    "input_layer = Input(shape=(INPUT_LENGTH ,))\n",
    "\n",
    "model = Sequential() #this will instantiate the sequrntial APIs and then we can add in the number of layers we need to build it up\n",
    "\n",
    "# Add the Input layer\n",
    "model.add(input_layer)\n",
    "\n",
    "#create the embedding layer, embedding is represented as number of words+1  +1 for unknow word. Our embedding will be 32 in length. i.e 32 features in the embedding for each word \n",
    "model.add(Embedding(MAX_FEATURES+1, 32))\n",
    "#create LSTM layer which will have 32 different lstm units and specify that they have activation of tan-h \n",
    "model.add(Bidirectional(LSTM(32, activation='tanh')))\n",
    "\n",
    "#3 layers for feature extraction\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "#final layers that will map to different output we got inside our neural network\n",
    "model.add(Dense(6, activation = 'sigmoid'))\n",
    "\n",
    "y[0]\n",
    "model.compile(loss='BinaryCrossentropy', optimizer='Adam')\n",
    "model.summary()\n",
    "create the training set \n",
    "history = model.fit(train, epochs=10, validation_data=val)\n",
    "#for training we run this .fit and pass training data, epochs specifies how long we want to train it or how many passes we are going to do for our training data and also pass validation data \n",
    "history.history\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "pd.DataFrame(history.history).plot()\n",
    "plt.show()\n",
    "Make predictions\n",
    "input_text = vectorizer('You freaking suck!')\n",
    "#use vectorizer and pass the text we want to vectorize \n",
    "#input text will be a sequence of integers which will be 1800 value long and if it does not meet with this length it will pad the results with 0\n",
    "now when we pass this to .predict as model.predict(input_text) then it will not work since we have not passed the input as a batch or series of values. \n",
    "\n",
    "since the model will expect the output to be like \n",
    "none x no. of val in our sequence x length of embedding/ input shape \n",
    "#create a batch variable to pass input in batch\n",
    "batch = test.as_numpy_iterator().next()\n",
    "#get data out on data pipeline\n",
    "batch_x, batch_y =  test.as_numpy_iterator().next()\n",
    "model.predict(batch_y)\n",
    "#here we can see all our comments are predicted.\n",
    "# to see only the ones with value 1 we can do \n",
    "(model.predict(batch_y) > 0.5).astype(int)\n",
    "#hence we pass it on custom dataset as \n",
    "model.predict(np.expand_dims(input_text,0))\n",
    "res = model.predict(np.expand_dims(input_text,0))\n",
    "# res will store our result and .predict will be used to make prediction\n",
    "evaluating the model\n",
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
    "#instiantiate precision matrix, recall and categorical accuracy \n",
    "pre = Precision() \n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()\n",
    "#making these predictions \n",
    "\n",
    "#loop through every single batch inside out data pipeline\n",
    "for batch in test.as_numpy_iterator():\n",
    "    #unpack the batch/values\n",
    "    x_true, y_true = batch\n",
    "\n",
    "    #make predictin and pass our comments x_true which are already tokenize \n",
    "    yhat = model.predict(x_true)\n",
    "\n",
    "    #flatten the prediction so that it become one big vector \n",
    "    y_true= y_true.flatten()\n",
    "    yhat = yhat.flatten()\n",
    "\n",
    "    #update the metrices based on current state of data\n",
    "    pre.update_state(y_true, yhat)\n",
    "    re.update_state(y_true, yhat)\n",
    "    acc.update_state(y_true, yhat)\n",
    "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()},Accuracy:{acc.result().numpy()}')\n",
    "build the gradio app\n",
    "%pip install gradio jinja2\n",
    "import gradio as gr\n",
    "model.save('Toxic_Comments.h5')\n",
    "#save the model in a file names Toxic_Comments in h5 format\n",
    "model = tf.keras.models.load_model('Toxic_Comments.h5')\n",
    "# load our model as backup \n",
    "input_str = vectorizer('hey I frekin hate you! I am coming for you and I will hurt you')\n",
    "res = model.predict(np.expand_dims( input_str, 0))\n",
    "df.columns[2:]\n",
    "res\n",
    "#implement a function that we will hook into our gradio model\n",
    "\n",
    "def score_comment(comment):\n",
    "    #pass the comment  through vectorizer so that it convert it into series of numbers\n",
    "    vectorized_comment = vectorizer([comment])\n",
    "    #run comments through our model\n",
    "    results = model.predict(vectorized_comment)\n",
    "\n",
    "    #unpack all our results \n",
    "    #this loop will go through each of our columns toxic, obscence etc one by one and result will be printed\n",
    "    text = ''\n",
    "    for idx, col in enumerate(df.columns[2:]):\n",
    "        text += '{}: {}\\n'.format(col, results[0][idx] > 0.5)\n",
    "    \n",
    "    return text\n",
    "interface = gr.Interface(fn = score_comment, #pass the function we want to run when we hit enter \n",
    "                         inputs = gr.Textbox(lines=2, placeholder = 'comment to score'), #input will be textbox \n",
    "                         outputs = 'text') #output will be set of text \n",
    "interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
